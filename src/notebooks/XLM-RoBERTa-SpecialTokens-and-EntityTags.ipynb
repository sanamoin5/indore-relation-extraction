{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\nImporting necessary python libraries\n'''\nimport random\nimport csv\nimport array\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom transformers import BertTokenizer, XLMRobertaTokenizer\nfrom transformers import XLMRobertaConfig\nfrom transformers import BertForSequenceClassification, XLMRobertaForSequenceClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-10T20:57:26.875083Z","iopub.execute_input":"2021-12-10T20:57:26.875856Z","iopub.status.idle":"2021-12-10T20:57:26.882521Z","shell.execute_reply.started":"2021-12-10T20:57:26.875828Z","shell.execute_reply":"2021-12-10T20:57:26.881614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetching training and validation data and converting it to the dataframe\ndf = pd.read_csv(\"/kaggle/input/indore-datathon-2021/train.tsv\", names=['Relation','Sentence','NER1','NER2'], sep=\"\\t\")\ndf.drop(index=df.index[0], axis=0, inplace=True)\n# df = df.drop(columns=['NER1', 'NER2'])\n\n# dfv = pd.read_csv(\"/kaggle/input/indore-datathon-2021/valid.tsv\", names=['Id','Sentence', 'NER1', 'NER2'], sep=\"\\t\")\n# dfv = dfv.drop(columns=['NER1', 'NER2'])\n\ndft = pd.read_csv(\"/kaggle/input/indore-datathon-2021/test.tsv\", names=['Id','Sentence','NER1','NER2'], sep=\"\\t\")\ndft.drop(index=dft.index[0], axis=0, inplace=True)\n# dft = dft.drop(columns=['NER1', 'NER2'])\n\n# Fetching extra labeled dump data for english (resource rich language)\ndf_english = pd.read_csv(\"/kaggle/input/indore-datathon-2021/en.tsv\", names=['Relation','Sentence','NER1','NER2'], sep=\"\\t\")\n# df_english = df_english.drop(columns=['NER1', 'NER2'])\ndf_english.drop(index=df_english.index[0], axis=0, inplace=True)\n\n# Fetching extra unlabeled dump data for all the languages\ndf_un_english = pd.read_csv(\"/kaggle/input/entire-dataset/english_annotated_dump.csv\", sep=\",\")\ndf_un_english = df_un_english.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_english = df_un_english.drop(columns=['NER1', 'NER2'])\ndf_un_english = df_un_english.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_english.drop(index=df_un_english.index[0], axis=0, inplace=True)\n\ndf_un_hindi = pd.read_csv(\"/kaggle/input/entire-dataset/hindi_annotated_dump.csv\", sep=\",\")\ndf_un_hindi = df_un_hindi.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_hindi = df_un_hindi.drop(columns=['NER1', 'NER2'])\ndf_un_hindi = df_un_hindi.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_hindi.drop(index=df_un_hindi.index[0], axis=0, inplace=True)\n\ndf_un_bengali = pd.read_csv(\"/kaggle/input/entire-dataset/bengali_annotated_dump.csv\", sep=\",\")\ndf_un_bengali = df_un_bengali.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_bengali = df_un_bengali.drop(columns=['NER1', 'NER2'])\ndf_un_bengali = df_un_bengali.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_bengali.drop(index=df_un_bengali.index[0], axis=0, inplace=True)\n\ndf_un_telugu = pd.read_csv(\"/kaggle/input/entire-dataset/telugu_annotated_dump.csv\", sep=\",\")\ndf_un_telugu = df_un_telugu.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_telugu = df_un_telugu.drop(columns=['NER1', 'NER2'])\ndf_un_telugu = df_un_telugu.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_telugu.drop(index=df_un_telugu.index[0], axis=0, inplace=True)\n\n# Fetching the previous validation data\ndfv = pd.read_csv(\"/kaggle/input/entire-dataset/RoBERTa_Without_Token_Submission_Valid.csv\", names=['Id', 'Relation','Sentence'], sep=\",\")\ndfv = dfv.drop(columns=['Id'])\ndfv.drop(index=dfv.index[0], axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:29.777209Z","iopub.execute_input":"2021-12-10T20:57:29.777454Z","iopub.status.idle":"2021-12-10T20:57:30.674564Z","shell.execute_reply.started":"2021-12-10T20:57:29.77743Z","shell.execute_reply":"2021-12-10T20:57:30.673827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis is a pre-processing step. \nThis function receives the raw data and add special tokens to the data as this is an essential step to train 'XLM-RoBERTa' model.\n''' \ndef tokenise_data(data):\n    tokenised_sentences = []\n    for sentence in data:\n        if sentence != 'Sentence':\n            tokenised_sentences.append(\"\".join(['<s>', sentence, '</s>']))\n        else:\n            tokenised_sentences.append(sentence)\n\n    data = tokenised_sentences\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:33.937711Z","iopub.execute_input":"2021-12-10T20:57:33.938414Z","iopub.status.idle":"2021-12-10T20:57:33.944617Z","shell.execute_reply.started":"2021-12-10T20:57:33.93838Z","shell.execute_reply":"2021-12-10T20:57:33.943412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis function is also part of the pre-processing step.\nThe function, takes in the dataframes and indexes the dataframe for better accessibility\n'''\ndef indexing_data_frame(dataframe):\n    dataframe['id'] = range(len(dataframe))\n    dataframe.set_index('id', inplace=True)\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:36.206529Z","iopub.execute_input":"2021-12-10T20:57:36.206756Z","iopub.status.idle":"2021-12-10T20:57:36.211028Z","shell.execute_reply.started":"2021-12-10T20:57:36.206732Z","shell.execute_reply":"2021-12-10T20:57:36.210237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis is also a pre-processing step.\nThe dataframes are passed as an argument to 'indexing_data_frame' function for indexing the dataframes\n'''\ndf = indexing_data_frame(df)\ndfv = indexing_data_frame(dfv)\ndft = indexing_data_frame(dft)\n\ndf_english = indexing_data_frame(df_english)\n\ndf_un_english = indexing_data_frame(df_un_english)\ndf_un_hindi = indexing_data_frame(df_un_hindi)\ndf_un_bengali = indexing_data_frame(df_un_bengali)\ndf_un_telugu = indexing_data_frame(df_un_telugu)\n# df_prev = indexing_data_frame(df_prev)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:38.486386Z","iopub.execute_input":"2021-12-10T20:57:38.487023Z","iopub.status.idle":"2021-12-10T20:57:38.497452Z","shell.execute_reply.started":"2021-12-10T20:57:38.486984Z","shell.execute_reply":"2021-12-10T20:57:38.496847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAs the extra english data is also labeled, this data is appended together with the training data for combined training.\nNOTE: Training Approaches:\n        1) Training the model with both 'train.tsv' and 'en.tsv'\n        2) Training the model with 'train.tsv'. Loading this model to train again with 'en.tsv'\n      Both of these approaches results in same final model. Hence, combining them togethor was much more feasibile.\n'''\ndf = df.append(df_english)\ndf = df.append(df_un_english)\ndf = df.append(df_un_hindi)\ndf = df.append(df_un_bengali)\ndf = df.append(df_un_telugu)\n# df = df.append(df_prev)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:41.063045Z","iopub.execute_input":"2021-12-10T20:57:41.064029Z","iopub.status.idle":"2021-12-10T20:57:41.092115Z","shell.execute_reply.started":"2021-12-10T20:57:41.063982Z","shell.execute_reply":"2021-12-10T20:57:41.090967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = indexing_data_frame(df)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:43.481896Z","iopub.execute_input":"2021-12-10T20:57:43.482367Z","iopub.status.idle":"2021-12-10T20:57:43.488429Z","shell.execute_reply.started":"2021-12-10T20:57:43.482338Z","shell.execute_reply":"2021-12-10T20:57:43.487414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_english.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:46.247375Z","iopub.execute_input":"2021-12-10T20:57:46.248327Z","iopub.status.idle":"2021-12-10T20:57:46.260172Z","shell.execute_reply.started":"2021-12-10T20:57:46.248234Z","shell.execute_reply":"2021-12-10T20:57:46.259415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, row in df.iterrows():\n    sentence = row['Sentence']\n    ner1 = row['NER1']\n    ner2 = row['NER2']\n    sentence = sentence.replace('</e1>', '</e1> ' + ner1)\n    sentence = sentence.replace('</e2>', '</e2> ' + ner2)\n    df['Sentence'][index] = sentence","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:57:49.338726Z","iopub.execute_input":"2021-12-10T20:57:49.33896Z","iopub.status.idle":"2021-12-10T20:58:18.24381Z","shell.execute_reply.started":"2021-12-10T20:57:49.338937Z","shell.execute_reply":"2021-12-10T20:58:18.243319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:58:18.257491Z","iopub.execute_input":"2021-12-10T20:58:18.257743Z","iopub.status.idle":"2021-12-10T20:58:18.27629Z","shell.execute_reply.started":"2021-12-10T20:58:18.257712Z","shell.execute_reply":"2021-12-10T20:58:18.275456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_counts = df.Relation.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.690879Z","iopub.execute_input":"2021-12-10T19:23:19.691149Z","iopub.status.idle":"2021-12-10T19:23:19.705659Z","shell.execute_reply.started":"2021-12-10T19:23:19.691114Z","shell.execute_reply":"2021-12-10T19:23:19.70486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_counts = relation_counts/max(relation_counts)\nrelation_tensor = relation_counts.values.reshape(25,1)\nrelation_counts = np.flip(relation_counts)\nrelation_counts = np.copy(relation_counts)\nrelation_tensor = torch.tensor(relation_counts)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.706904Z","iopub.execute_input":"2021-12-10T19:23:19.707524Z","iopub.status.idle":"2021-12-10T19:23:19.715096Z","shell.execute_reply.started":"2021-12-10T19:23:19.707484Z","shell.execute_reply":"2021-12-10T19:23:19.714337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_tensor","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.716207Z","iopub.execute_input":"2021-12-10T19:23:19.717045Z","iopub.status.idle":"2021-12-10T19:23:19.726406Z","shell.execute_reply.started":"2021-12-10T19:23:19.716962Z","shell.execute_reply":"2021-12-10T19:23:19.725394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis is a pre-processing step. \nThe data is sent to the 'tokenise_data' function to add the special tokens required for training.\n'''\ndf.Sentence = tokenise_data(df.Sentence)\ndfv.Sentence = tokenise_data(dfv.Sentence)\ndft.Sentence = tokenise_data(dft.Sentence)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.730606Z","iopub.execute_input":"2021-12-10T19:23:19.730944Z","iopub.status.idle":"2021-12-10T19:23:19.800109Z","shell.execute_reply.started":"2021-12-10T19:23:19.730912Z","shell.execute_reply":"2021-12-10T19:23:19.799347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfv","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.803357Z","iopub.execute_input":"2021-12-10T19:23:19.803596Z","iopub.status.idle":"2021-12-10T19:23:19.816824Z","shell.execute_reply.started":"2021-12-10T19:23:19.803564Z","shell.execute_reply":"2021-12-10T19:23:19.815855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nConvert relation labels to integer values\n'''\nlabels = df.Relation.unique()\nlabel_dict = {label:index for index, label in enumerate(labels)}","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.818453Z","iopub.execute_input":"2021-12-10T19:23:19.819329Z","iopub.status.idle":"2021-12-10T19:23:19.830173Z","shell.execute_reply.started":"2021-12-10T19:23:19.819284Z","shell.execute_reply":"2021-12-10T19:23:19.829503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAdding the numerical relation values to the dataframe of training data\n'''\ndf['Relation'] = df['Relation'].map(label_dict)\ndfv['Relation'] = dfv['Relation'].map(label_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.831293Z","iopub.execute_input":"2021-12-10T19:23:19.831674Z","iopub.status.idle":"2021-12-10T19:23:19.851804Z","shell.execute_reply.started":"2021-12-10T19:23:19.831636Z","shell.execute_reply":"2021-12-10T19:23:19.850934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nSpliting up the data randomly into test and validation set.\nAs of now, Gold Standard dataset is not available. \nHence we consider training data for both training and validation and validation data as a test data.\n'''\n# X_train, X_val, y_train, y_val = train_test_split(df.index.values, df.Relation.values, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.853196Z","iopub.execute_input":"2021-12-10T19:23:19.854013Z","iopub.status.idle":"2021-12-10T19:23:19.860975Z","shell.execute_reply.started":"2021-12-10T19:23:19.853976Z","shell.execute_reply":"2021-12-10T19:23:19.860094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nPre-trained model types depending on the vocabulary size. We do not use the entire list of pre-trained configuration. \nFor more information, refer to the following link: \nhttps://huggingface.co/transformers/_modules/transformers/models/xlm_roberta/tokenization_xlm_roberta.html#XLMRobertaTokenizer\n\nChoose from 2 different model-types: 'xlm-roberta-base' or 'xlm-roberta-large'\n'''\nmodel_type = 'xlm-roberta-base'\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_type)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:19.862556Z","iopub.execute_input":"2021-12-10T19:23:19.862969Z","iopub.status.idle":"2021-12-10T19:23:21.266849Z","shell.execute_reply.started":"2021-12-10T19:23:19.862933Z","shell.execute_reply":"2021-12-10T19:23:21.26602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nRefer https://github.com/huggingface/tokenizers/issues/247 for more information regarding what special tokens are meant to do and \nhow fine-tuning the model can help create better embeddings with these newly added tokens.\n'''\nspecial_tokens_dict = {'additional_special_tokens': ['<e1>', '</e1>', '<e2>', '</e2>']}\ntokenizer.add_special_tokens(special_tokens_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:21.269837Z","iopub.execute_input":"2021-12-10T19:23:21.270131Z","iopub.status.idle":"2021-12-10T19:23:21.276059Z","shell.execute_reply.started":"2021-12-10T19:23:21.270092Z","shell.execute_reply":"2021-12-10T19:23:21.275336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAnother pre-processing step before we can train the model.\nModel training or fine-tuning requires the encoding of the data to the respective tensor format.\nThis function takes in the data as an arguments, encodes the data and returns the data in the tensor format.\n'''\ndef encode_data(data):\n    encoded_data = tokenizer.batch_encode_plus(\n                        data,\n                        add_special_tokens=False,\n        \n                        max_length=512,\n                        padding='max_length',\n                        truncation=True,\n\n                        return_attention_mask=True,\n                        return_tensors='pt'\n                    )\n    return encoded_data","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:21.277325Z","iopub.execute_input":"2021-12-10T19:23:21.278387Z","iopub.status.idle":"2021-12-10T19:23:21.286127Z","shell.execute_reply.started":"2021-12-10T19:23:21.278279Z","shell.execute_reply":"2021-12-10T19:23:21.285202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAnother pre-processing step before training the model.\nModel training or fine-tuning requires the encoding of the data to the respective tensor format.\nHence, the data is passed on to 'encode_data' function, which returns the data in the respective tensor format.\n'''\nencoded_data_training = encode_data(df.Sentence.values)\nencoded_data_val = encode_data(dfv.Sentence.values)\nencoded_data_test = encode_data(dft.Sentence.values)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:23:21.288001Z","iopub.execute_input":"2021-12-10T19:23:21.288406Z","iopub.status.idle":"2021-12-10T19:24:00.704118Z","shell.execute_reply.started":"2021-12-10T19:23:21.288366Z","shell.execute_reply":"2021-12-10T19:24:00.703385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFetch different layers of the encoded data required for training and prediction\nThis function takes in \n    'encoded_data', 'dataframe' and 'type' of data to \n        return \n    'input_ids', 'attention_mask' and 'labels' where applicable.\n'''\ndef fetch_data_layers(encoded_data, dataframe, type):\n    if type=='train' or type=='val':\n        input_ids = encoded_data['input_ids']\n        attention_masks = encoded_data['attention_mask']\n        labels = torch.tensor(dataframe.Relation.values)\n        return input_ids, attention_masks, labels\n    else:\n        input_ids = encoded_data['input_ids']\n        attention_masks = encoded_data['attention_mask']\n        return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:00.705615Z","iopub.execute_input":"2021-12-10T19:24:00.706041Z","iopub.status.idle":"2021-12-10T19:24:00.712994Z","shell.execute_reply.started":"2021-12-10T19:24:00.706002Z","shell.execute_reply":"2021-12-10T19:24:00.712271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThe 'encoded_data', 'dataframe' and 'type' of data are passed to function 'fetch_data_layers' which returns \n'input_ids', 'attention_mask' and 'labels' where applicable.\n'''\ninput_ids_train, attention_masks_train, labels_train = fetch_data_layers(encoded_data_training, df,  'train')\ninput_ids_val, attention_masks_val, labels_val = fetch_data_layers(encoded_data_val, dfv,  'val')\ninput_ids_test, attention_masks_test = fetch_data_layers(encoded_data_test, dft,  'test')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:00.714602Z","iopub.execute_input":"2021-12-10T19:24:00.715062Z","iopub.status.idle":"2021-12-10T19:24:00.724299Z","shell.execute_reply.started":"2021-12-10T19:24:00.715024Z","shell.execute_reply":"2021-12-10T19:24:00.72359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nBasic configuration for training the model\n'''\nbatch_size = 8\nepochs = 5","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:00.725754Z","iopub.execute_input":"2021-12-10T19:24:00.726072Z","iopub.status.idle":"2021-12-10T19:24:00.735601Z","shell.execute_reply.started":"2021-12-10T19:24:00.726031Z","shell.execute_reply":"2021-12-10T19:24:00.734758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(labels_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:00.736872Z","iopub.execute_input":"2021-12-10T19:24:00.737157Z","iopub.status.idle":"2021-12-10T19:24:00.747943Z","shell.execute_reply.started":"2021-12-10T19:24:00.737121Z","shell.execute_reply":"2021-12-10T19:24:00.747221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreating the dataset from the fetched layers\n'''\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\ndataset_test = TensorDataset(input_ids_test, attention_masks_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:00.749366Z","iopub.execute_input":"2021-12-10T19:24:00.750256Z","iopub.status.idle":"2021-12-10T19:24:00.755473Z","shell.execute_reply.started":"2021-12-10T19:24:00.750216Z","shell.execute_reply":"2021-12-10T19:24:00.75457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreating the dataloader by randomly sampling using the dataset previously created through the fetched layers.\n'''\ndataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\ndataloader_val = DataLoader(dataset_val, sampler=RandomSampler(dataset_val), batch_size=batch_size)\ndataloader_test = DataLoader(dataset_test, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:00.756845Z","iopub.execute_input":"2021-12-10T19:24:00.757446Z","iopub.status.idle":"2021-12-10T19:24:00.768278Z","shell.execute_reply.started":"2021-12-10T19:24:00.757409Z","shell.execute_reply":"2021-12-10T19:24:00.76736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nLoading the pre-trained 'XLM-RoBERTa' model and passing the number of labels along with it as a configuration step.\n'''\n# config = XLMRobertaConfig.from_pretrained(model_type)\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_type, num_labels=len(label_dict))\nmodel.resize_token_embeddings(len(tokenizer))\n# model.load_state_dict(torch.load('/kaggle/working/Roberta_Model_Checkpoint.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:00.769557Z","iopub.execute_input":"2021-12-10T19:24:00.770517Z","iopub.status.idle":"2021-12-10T19:24:04.522913Z","shell.execute_reply.started":"2021-12-10T19:24:00.770476Z","shell.execute_reply":"2021-12-10T19:24:04.522217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"softmax_layer = nn.LogSoftmax(dim=1)\nentropy_loss = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.524052Z","iopub.execute_input":"2021-12-10T19:24:04.524388Z","iopub.status.idle":"2021-12-10T19:24:04.528765Z","shell.execute_reply.started":"2021-12-10T19:24:04.524349Z","shell.execute_reply":"2021-12-10T19:24:04.528083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nBasic configuration for training the model\n'''\noptimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.530167Z","iopub.execute_input":"2021-12-10T19:24:04.530787Z","iopub.status.idle":"2021-12-10T19:24:04.543065Z","shell.execute_reply.started":"2021-12-10T19:24:04.530751Z","shell.execute_reply":"2021-12-10T19:24:04.542413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.547526Z","iopub.execute_input":"2021-12-10T19:24:04.547937Z","iopub.status.idle":"2021-12-10T19:24:04.552118Z","shell.execute_reply.started":"2021-12-10T19:24:04.547908Z","shell.execute_reply":"2021-12-10T19:24:04.551134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_per_class(preds, labels):\n    label_dict_inverse = {v:k for k,v in label_dict.items()}\n\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.553403Z","iopub.execute_input":"2021-12-10T19:24:04.553863Z","iopub.status.idle":"2021-12-10T19:24:04.566224Z","shell.execute_reply.started":"2021-12-10T19:24:04.553828Z","shell.execute_reply":"2021-12-10T19:24:04.565582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_val = 123\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.56841Z","iopub.execute_input":"2021-12-10T19:24:04.569017Z","iopub.status.idle":"2021-12-10T19:24:04.57689Z","shell.execute_reply.started":"2021-12-10T19:24:04.568978Z","shell.execute_reply":"2021-12-10T19:24:04.576067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.579755Z","iopub.execute_input":"2021-12-10T19:24:04.580268Z","iopub.status.idle":"2021-12-10T19:24:04.889081Z","shell.execute_reply.started":"2021-12-10T19:24:04.58023Z","shell.execute_reply":"2021-12-10T19:24:04.88823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(dataloader_test):\n\n    model.eval()\n    \n    predictions = []\n    \n    for batch in tqdm(dataloader_test):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1]\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        logits = outputs['logits']\n\n        logits = logits.detach().cpu().numpy()\n        predictions.append(logits)\n    \n    predictions = np.concatenate(predictions, axis=0)\n            \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.891346Z","iopub.execute_input":"2021-12-10T19:24:04.891765Z","iopub.status.idle":"2021-12-10T19:24:04.89782Z","shell.execute_reply.started":"2021-12-10T19:24:04.891725Z","shell.execute_reply":"2021-12-10T19:24:04.897109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2]\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.899223Z","iopub.execute_input":"2021-12-10T19:24:04.899711Z","iopub.status.idle":"2021-12-10T19:24:04.909476Z","shell.execute_reply.started":"2021-12-10T19:24:04.899676Z","shell.execute_reply":"2021-12-10T19:24:04.908786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_tensor","metadata":{"execution":{"iopub.status.busy":"2021-12-10T19:24:04.910795Z","iopub.execute_input":"2021-12-10T19:24:04.9113Z","iopub.status.idle":"2021-12-10T19:24:04.925177Z","shell.execute_reply.started":"2021-12-10T19:24:04.911264Z","shell.execute_reply":"2021-12-10T19:24:04.92444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(1, epochs+1)):\n    model.train()\n    loss_train_total = 0\n    best_validation_loss = float('inf')\n    \n    progress_bar = tqdm(dataloader_train, \n                        desc='Epoch {:1d}'.format(epoch), \n                        leave=False, \n                        disable=False)\n    \n    for batch in progress_bar:\n        model.zero_grad()\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {\n            'input_ids':        batch[0],\n            'attention_mask':   batch[1],\n            'labels':           batch[2],\n        }\n        # inputs['input_ids'].to(device)\n        # inputs['attention_mask'].to(device)\n        # inputs['labels'].to(device)\n\n        # outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['labels'])\n        outputs = model(**inputs)        \n        loss = outputs[0]\n        logits = outputs[1]\n        relation_tensor = relation_tensor.type(torch.DoubleTensor)\n        relation_tensor = relation_tensor.to(device)\n        \n        logits = logits.type(torch.DoubleTensor)\n        logits = logits.to(device)\n        new_logits = torch.tensor([])\n        new_logits = new_logits.to(device)\n        new_logits = logits * relation_tensor\n\n        \n        ll = entropy_loss(new_logits, inputs['labels'])\n        \n        # loss_train_total +=loss.item()\n        loss_train_total +=ll.item()\n        # loss.backward()\n        ll.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        # progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(ll.item()/len(batch))})     \n    \n    \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total/len(dataloader_train)\n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (weighted): {val_f1}')\n    \n    if val_loss <= best_validation_loss:\n        torch.save(model.state_dict(), 'Roberta_Model_Checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:20:51.889967Z","iopub.execute_input":"2021-12-10T20:20:51.890481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_per_class(predictions, true_vals)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:20:37.098649Z","iopub.execute_input":"2021-12-10T20:20:37.099128Z","iopub.status.idle":"2021-12-10T20:20:37.125349Z","shell.execute_reply.started":"2021-12-10T20:20:37.099091Z","shell.execute_reply":"2021-12-10T20:20:37.124482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predict(dataloader_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:20:37.126408Z","iopub.status.idle":"2021-12-10T20:20:37.127269Z","shell.execute_reply.started":"2021-12-10T20:20:37.127004Z","shell.execute_reply":"2021-12-10T20:20:37.127032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_flat = np.argmax(predictions, axis=1).flatten()\n# len(preds_flat)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:20:37.128781Z","iopub.status.idle":"2021-12-10T20:20:37.129179Z","shell.execute_reply.started":"2021-12-10T20:20:37.12896Z","shell.execute_reply":"2021-12-10T20:20:37.128983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict_rev = dict((v,k) for k,v in label_dict.items())","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:20:37.13106Z","iopub.status.idle":"2021-12-10T20:20:37.131738Z","shell.execute_reply.started":"2021-12-10T20:20:37.13147Z","shell.execute_reply":"2021-12-10T20:20:37.131496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dft['predicted_relations_id'] = preds_flat\ndft['predicted_relations'] = dft['predicted_relations_id'].map(label_dict_rev)\ndft.to_csv('Roberta_With_Tokens.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T20:20:37.132958Z","iopub.status.idle":"2021-12-10T20:20:37.133613Z","shell.execute_reply.started":"2021-12-10T20:20:37.133351Z","shell.execute_reply":"2021-12-10T20:20:37.133377Z"},"trusted":true},"execution_count":null,"outputs":[]}]}