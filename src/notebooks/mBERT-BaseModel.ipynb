{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"id":"OLsss4JrrKw8","outputId":"0873c6db-5127-4ea7-840e-235bc415e086"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"PpPKoKIOrhwL","outputId":"fc72cd11-3d9e-4c4b-ba15-9e0215144a4a","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler","metadata":{"id":"dnXpYvwcrUqe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"train.tsv\", names=['Relation','Sentence','NER1','NER2'], sep=\"\\t\")\ndf['id'] = range(len(df))\ndf.set_index('id', inplace=True)\ndf.head()","metadata":{"id":"DSvwzbYEreHd","outputId":"098984cf-abe3-4333-cd44-2ab5453386d4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Relation.value_counts()","metadata":{"id":"eWnMDAhLsA9K","outputId":"d28abf49-f560-4550-a653-fbb4fb77296b","tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert relation labels to integer values\nlabels = df.Relation.unique()\n\nlabel_dict = {label:index for index, label in enumerate(labels)}","metadata":{"id":"iCh4EM9LsXeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Relation'] = df['Relation'].map(label_dict)\ndf.head()","metadata":{"id":"XDr-cMoetEo3","outputId":"7da24a75-3668-472b-e8b5-33e462ca86e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_val = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/DL4NLP/data/valid.tsv\", names=['Relation','Sentence','NER1','NER2'], sep=\"\\t\")\n# df_val['Relation'] = df_val['Relation'].map(label_dict)\n\nX_train, X_val, y_train, y_val = train_test_split(df.index.values, df.Relation.values, test_size=0.01, random_state=42)","metadata":{"id":"Nm2wB1-htLIa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here they also add a label to indicate whether it is a training, validation or test set item.\n# if you do cross-validation, you do not need such labeling.\ndf['data_type'] = ['not_set']*df.shape[0]\n\ndf.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'\n\ndf.groupby(['Relation', 'data_type']).count()","metadata":{"id":"6GJ7-B-Jy5YC","outputId":"4c4c066c-02a8-478b-d773-9090bd3da999"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_special_tokens(self):\n        special_tokens_dict = {'additional_special_tokens': ['<e1>', '</e1>', '<e2>', '</e2>']}\n        self.add_special_tokens(special_tokens_dict)\n        self.model.resize_token_embeddings(len(self.tokenizer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')","metadata":{"id":"KA5a2Y2lteQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens_dict = {'additional_special_tokens': ['<e1>', '</e1>', '<e2>', '</e2>']}\ntokenizer.add_special_tokens(special_tokens_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Sentence = df.Sentence.apply(lambda x: \"\".join(['<s>', x, '</s>']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_data_train = tokenizer.batch_encode_plus(\n    df.Sentence.values,\n    add_special_tokens=False,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=256,\n    return_tensors='pt'\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    df.Sentence.values,\n    add_special_tokens=False,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=256,\n    return_tensors='pt'\n)\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(df.Relation.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df.Relation.values)","metadata":{"id":"5RYmGDi6tfNU","outputId":"118157b3-2323-437c-ee7b-46c130f5c933"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_data_val['input_ids'].shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","metadata":{"id":"1XdJm0kvvzU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=len(label_dict), output_attentions=False, output_hidden_states=False)","metadata":{"id":"aYCyKuSZw2aZ","outputId":"00a6940c-1f21-45e7-d5f9-b3ccb1b00d2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\n\ndataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\ndataloader_val = DataLoader(dataset_val, sampler=RandomSampler(dataset_val), batch_size=8)","metadata":{"id":"w_690DumvJyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)","metadata":{"id":"9WwLFodWwqNv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\n\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)","metadata":{"id":"qwSzbRrewwbK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","metadata":{"id":"an3SmhTMxR0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_per_class(preds, labels):\n    label_dict_inverse = {v:k for k,v in label_dict.items()}\n\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')","metadata":{"id":"JCWJK70exh-c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nseed_val = 123\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"id":"I9PHnf9CyOoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(device)","metadata":{"id":"Rm-b941Eyd-2","outputId":"1f694c55-ece5-4d04-ee25-f51888a72bb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions = []\n    true_vals = []\n    \n    for batch in tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2]\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","metadata":{"id":"B2zyUTWhymnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" model.resize_token_embeddings(len(tokenizer))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(1, epochs+1)):\n    model.train()\n    loss_train_total = 0\n    \n    progress_bar = tqdm(dataloader_train, \n                        desc='Epoch {:1d}'.format(epoch), \n                        leave=False, \n                        disable=False)\n    \n    for batch in progress_bar:\n        model.zero_grad()\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {\n            'input_ids':        batch[0],\n            'attention_mask':   batch[1],\n            'labels':           batch[2],\n        }\n        \n        outputs = model(**inputs)\n        loss = outputs[0]\n        loss_train_total +=loss.item()\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     \n    \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total/len(dataloader_train)\n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (weighted): {val_f1}')","metadata":{"id":"Avk4TAsC0VFy","outputId":"076bd918-8338-49a4-bcf4-8be6dcbbd90e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_per_class(predictions, true_vals)","metadata":{"id":"8sOMFSwt1sYy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------------------------------------------------\n-----------------------------------------------------------------------------\n-----------------------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"dfv = pd.read_csv(\"valid.tsv\", sep=\"\\t\")\ndfv['id'] = range(len(dfv))\ndfv.set_index('Id', inplace=True)\ndfv.head()","metadata":{"id":"DSvwzbYEreHd","outputId":"098984cf-abe3-4333-cd44-2ab5453386d4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfv.Sentence = dfv.Sentence.apply(lambda x: \"\".join(['<s>', x, '</s>']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_data_val = tokenizer.batch_encode_plus(\n    dfv.Sentence.values,\n    add_special_tokens=False,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=256,\n    return_tensors='pt'\n)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\n\n\ndataset_val = TensorDataset(input_ids_val, attention_masks_val)\ndataloader_val = DataLoader(dataset_val, sampler=None, batch_size=8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_val(dataloader_val):\n\n    model.eval()\n    predictions = []\n  \n    for batch in tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1]                  \n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n\n        \n        logits = outputs[0]\n \n\n        logits = logits.detach().cpu().numpy()\n        predictions.append(logits)\n    \n    \n    predictions = np.concatenate(predictions, axis=0)\n            \n    return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = evaluate_val(dataloader_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds2 = np.argmax(preds,axis=1)\npreds2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict_rev = dict((v,k) for k,v in label_dict.items())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfv['preds'] = preds2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfv['preds'] = dfv['preds'].map(label_dict_rev)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfv.to_csv('team_uhh_predictions.csv')","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}