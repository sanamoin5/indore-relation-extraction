{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # for TPU\n# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T22:47:12.972125Z","iopub.execute_input":"2021-12-12T22:47:12.972702Z","iopub.status.idle":"2021-12-12T22:47:12.993883Z","shell.execute_reply.started":"2021-12-12T22:47:12.972589Z","shell.execute_reply":"2021-12-12T22:47:12.993284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nImporting necessary python libraries\n'''\nimport random\nimport math\nimport csv\nimport array\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom transformers import BertTokenizer, XLMRobertaTokenizer\nfrom transformers import XLMRobertaConfig\nfrom transformers import BertForSequenceClassification, XLMRobertaForSequenceClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# # for TPU\n# import torch_xla\n# import torch_xla.core.xla_model as xm","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:12.995675Z","iopub.execute_input":"2021-12-12T22:47:12.995968Z","iopub.status.idle":"2021-12-12T22:47:20.079612Z","shell.execute_reply.started":"2021-12-12T22:47:12.995933Z","shell.execute_reply":"2021-12-12T22:47:20.078879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetching training and validation data and converting it to the dataframe\n# df = pd.read_csv(\"/kaggle/input/indore-datathon-2021/train.tsv\", names=['Relation','Sentence','NER1','NER2'], sep=\"\\t\")\n# df.drop(index=df.index[0], axis=0, inplace=True)\n# df = df.drop(columns=['NER1', 'NER2'])\n\n# dfv = pd.read_csv(\"/kaggle/input/indore-datathon-2021/valid.tsv\", names=['Id','Sentence', 'NER1', 'NER2'], sep=\"\\t\")\n# dfv = dfv.drop(columns=['NER1', 'NER2'])\n\ndft = pd.read_csv(\"/kaggle/input/indore-datathon-2021/test.tsv\", names=['Id','Sentence','NER1','NER2'], sep=\"\\t\")\ndft.drop(index=dft.index[0], axis=0, inplace=True)\n# dft = dft.drop(columns=['NER1', 'NER2'])\n\n# Fetching extra labeled dump data for english (resource rich language)\ndf_english = pd.read_csv(\"/kaggle/input/indore-datathon-2021/en.tsv\", names=['Relation','Sentence','NER1','NER2'], sep=\"\\t\")\n# df_english = df_english.drop(columns=['NER1', 'NER2'])\ndf_english.drop(index=df_english.index[0], axis=0, inplace=True)\n\n# Fetching extra unlabeled dump data for all the languages\ndf_un_english = pd.read_csv(\"/kaggle/input/entire-dataset/english_annotated_dump.csv\", sep=\",\")\ndf_un_english = df_un_english.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_english = df_un_english.drop(columns=['NER1', 'NER2'])\ndf_un_english = df_un_english.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_english.drop(index=df_un_english.index[0], axis=0, inplace=True)\n\ndf_un_hindi = pd.read_csv(\"/kaggle/input/entire-dataset/hindi_annotated_dump.csv\", sep=\",\")\ndf_un_hindi = df_un_hindi.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_hindi = df_un_hindi.drop(columns=['NER1', 'NER2'])\ndf_un_hindi = df_un_hindi.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_hindi.drop(index=df_un_hindi.index[0], axis=0, inplace=True)\n\ndf_un_bengali = pd.read_csv(\"/kaggle/input/entire-dataset/bengali_annotated_dump.csv\", sep=\",\")\ndf_un_bengali = df_un_bengali.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_bengali = df_un_bengali.drop(columns=['NER1', 'NER2'])\ndf_un_bengali = df_un_bengali.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_bengali.drop(index=df_un_bengali.index[0], axis=0, inplace=True)\n\ndf_un_telugu = pd.read_csv(\"/kaggle/input/entire-dataset/telugu_annotated_dump.csv\", sep=\",\")\ndf_un_telugu = df_un_telugu.drop(columns=['Sentence', 'e1', 'e2', 'Unnamed: 0'])\n# df_un_telugu = df_un_telugu.drop(columns=['NER1', 'NER2'])\ndf_un_telugu = df_un_telugu.rename(columns={'Sentence_tagged':'Sentence'})\ndf_un_telugu.drop(index=df_un_telugu.index[0], axis=0, inplace=True)\n\n# Fetching the previous validation data\ndfv = pd.read_csv(\"/kaggle/input/entire-dataset/RoBERTa_Without_Token_Submission_Valid.csv\", names=['Id', 'Relation','Sentence'], sep=\",\")\ndfv = dfv.drop(columns=['Id'])\ndfv.drop(index=dfv.index[0], axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:20.081033Z","iopub.execute_input":"2021-12-12T22:47:20.081454Z","iopub.status.idle":"2021-12-12T22:47:21.753751Z","shell.execute_reply.started":"2021-12-12T22:47:20.081417Z","shell.execute_reply":"2021-12-12T22:47:21.753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis is a pre-processing step. \nThis function receives the raw data and add special tokens to the data as this is an essential step to train 'XLM-RoBERTa' model.\n''' \ndef tokenise_data(data):\n    tokenised_sentences = []\n    for sentence in data:\n        if sentence != 'Sentence':\n            tokenised_sentences.append(\"\".join(['<s>', sentence, '</s>']))\n        else:\n            tokenised_sentences.append(sentence)\n\n    data = tokenised_sentences\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.755969Z","iopub.execute_input":"2021-12-12T22:47:21.756402Z","iopub.status.idle":"2021-12-12T22:47:21.76203Z","shell.execute_reply.started":"2021-12-12T22:47:21.756366Z","shell.execute_reply":"2021-12-12T22:47:21.761209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis function is also part of the pre-processing step.\nThe function, takes in the dataframes and indexes the dataframe for better accessibility\n'''\ndef indexing_data_frame(dataframe):\n    dataframe['id'] = range(len(dataframe))\n    dataframe.set_index('id', inplace=True)\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.765024Z","iopub.execute_input":"2021-12-12T22:47:21.765369Z","iopub.status.idle":"2021-12-12T22:47:21.775419Z","shell.execute_reply.started":"2021-12-12T22:47:21.765333Z","shell.execute_reply":"2021-12-12T22:47:21.774649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis is also a pre-processing step.\nThe dataframes are passed as an argument to 'indexing_data_frame' function for indexing the dataframes\n'''\n# df = indexing_data_frame(df)\ndfv = indexing_data_frame(dfv)\ndft = indexing_data_frame(dft)\n\ndf_english = indexing_data_frame(df_english)\n\ndf_un_english = indexing_data_frame(df_un_english)\ndf_un_hindi = indexing_data_frame(df_un_hindi)\ndf_un_bengali = indexing_data_frame(df_un_bengali)\ndf_un_telugu = indexing_data_frame(df_un_telugu)\n# df_prev = indexing_data_frame(df_prev)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.776673Z","iopub.execute_input":"2021-12-12T22:47:21.777388Z","iopub.status.idle":"2021-12-12T22:47:21.792179Z","shell.execute_reply.started":"2021-12-12T22:47:21.77735Z","shell.execute_reply":"2021-12-12T22:47:21.791231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAs the extra english data is also labeled, this data is appended together with the training data for combined training.\nNOTE: Training Approaches:\n        1) Training the model with both 'train.tsv' and 'en.tsv'\n        2) Training the model with 'train.tsv'. Loading this model to train again with 'en.tsv'\n      Both of these approaches results in same final model. Hence, combining them togethor was much more feasibile.\n'''\ndf = df_un_english\n# df = df.append(df_english)\n# df = df.append(df_un_english)\ndf = df.append(df_un_hindi)\ndf = df.append(df_un_bengali)\ndf = df.append(df_un_telugu)\n# df = df.append(df_prev)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.794202Z","iopub.execute_input":"2021-12-12T22:47:21.794647Z","iopub.status.idle":"2021-12-12T22:47:21.809329Z","shell.execute_reply.started":"2021-12-12T22:47:21.794586Z","shell.execute_reply":"2021-12-12T22:47:21.808701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_class_weight(labels_dict,mu=0.1):\n    total = np.sum(list(labels_dict.values()))\n    keys = labels_dict.keys()\n    class_weight = dict()\n    \n    for key in keys:\n#         score = math.log(mu*total/float(labels_dict[key]))\n        score = (mu*total/float(labels_dict[key]))\n        class_weight[key] = score \n    \n    return class_weight","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.810478Z","iopub.execute_input":"2021-12-12T22:47:21.810715Z","iopub.status.idle":"2021-12-12T22:47:21.816434Z","shell.execute_reply.started":"2021-12-12T22:47:21.810682Z","shell.execute_reply":"2021-12-12T22:47:21.815577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# relation_counts = relation_counts/max(relation_counts)\n# relation_tensor = relation_counts.values.reshape(25,1)\n# relation_counts = np.flip(relation_counts)\n# relation_counts = np.copy(relation_counts)\n# relation_tensor = torch.tensor(relation_counts)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.818249Z","iopub.execute_input":"2021-12-12T22:47:21.818525Z","iopub.status.idle":"2021-12-12T22:47:21.825796Z","shell.execute_reply.started":"2021-12-12T22:47:21.818465Z","shell.execute_reply":"2021-12-12T22:47:21.825064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nThis is a pre-processing step. \nThe data is sent to the 'tokenise_data' function to add the special tokens required for training.\n'''\ndf.Sentence = tokenise_data(df.Sentence)\ndfv.Sentence = tokenise_data(dfv.Sentence)\ndft.Sentence = tokenise_data(dft.Sentence)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.827154Z","iopub.execute_input":"2021-12-12T22:47:21.827432Z","iopub.status.idle":"2021-12-12T22:47:21.882937Z","shell.execute_reply.started":"2021-12-12T22:47:21.827384Z","shell.execute_reply":"2021-12-12T22:47:21.882167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfv","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.885266Z","iopub.execute_input":"2021-12-12T22:47:21.885464Z","iopub.status.idle":"2021-12-12T22:47:21.904809Z","shell.execute_reply.started":"2021-12-12T22:47:21.88544Z","shell.execute_reply":"2021-12-12T22:47:21.904137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Relation.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.906977Z","iopub.execute_input":"2021-12-12T22:47:21.907649Z","iopub.status.idle":"2021-12-12T22:47:21.923711Z","shell.execute_reply.started":"2021-12-12T22:47:21.907605Z","shell.execute_reply":"2021-12-12T22:47:21.922924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Relation.unique()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.92521Z","iopub.execute_input":"2021-12-12T22:47:21.925944Z","iopub.status.idle":"2021-12-12T22:47:21.938648Z","shell.execute_reply.started":"2021-12-12T22:47:21.925903Z","shell.execute_reply":"2021-12-12T22:47:21.937587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nConvert relation labels to integer values\n'''\nlabels = df.Relation.unique()\nlabel_dict = {label:index for index, label in enumerate(labels)}","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.945234Z","iopub.execute_input":"2021-12-12T22:47:21.945468Z","iopub.status.idle":"2021-12-12T22:47:21.954264Z","shell.execute_reply.started":"2021-12-12T22:47:21.945442Z","shell.execute_reply":"2021-12-12T22:47:21.953113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_counts = df.Relation.value_counts()\nlabel_weight_dict = {label:relation_counts[label] for index, label in enumerate(labels)}","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.955924Z","iopub.execute_input":"2021-12-12T22:47:21.956469Z","iopub.status.idle":"2021-12-12T22:47:21.96971Z","shell.execute_reply.started":"2021-12-12T22:47:21.95643Z","shell.execute_reply":"2021-12-12T22:47:21.968954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_weight_dict","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.971127Z","iopub.execute_input":"2021-12-12T22:47:21.971537Z","iopub.status.idle":"2021-12-12T22:47:21.982942Z","shell.execute_reply.started":"2021-12-12T22:47:21.971486Z","shell.execute_reply":"2021-12-12T22:47:21.981051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = create_class_weight(label_weight_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.984049Z","iopub.execute_input":"2021-12-12T22:47:21.984386Z","iopub.status.idle":"2021-12-12T22:47:21.990546Z","shell.execute_reply.started":"2021-12-12T22:47:21.984346Z","shell.execute_reply":"2021-12-12T22:47:21.989541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = list(weights.values())","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:21.992473Z","iopub.execute_input":"2021-12-12T22:47:21.992807Z","iopub.status.idle":"2021-12-12T22:47:21.998594Z","shell.execute_reply.started":"2021-12-12T22:47:21.992768Z","shell.execute_reply":"2021-12-12T22:47:21.997309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(weights)):\n    weights[i] = weights[i]/max(weights)\n    \nweights","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:22.000091Z","iopub.execute_input":"2021-12-12T22:47:22.000409Z","iopub.status.idle":"2021-12-12T22:47:22.009337Z","shell.execute_reply.started":"2021-12-12T22:47:22.000373Z","shell.execute_reply":"2021-12-12T22:47:22.008421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_tensor = torch.tensor(weights)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:22.010796Z","iopub.execute_input":"2021-12-12T22:47:22.011382Z","iopub.status.idle":"2021-12-12T22:47:22.021897Z","shell.execute_reply.started":"2021-12-12T22:47:22.011341Z","shell.execute_reply":"2021-12-12T22:47:22.021212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_tensor","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:22.023136Z","iopub.execute_input":"2021-12-12T22:47:22.023882Z","iopub.status.idle":"2021-12-12T22:47:22.083787Z","shell.execute_reply.started":"2021-12-12T22:47:22.023827Z","shell.execute_reply":"2021-12-12T22:47:22.082951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nAdding the numerical relation values to the dataframe of training data\n'''\ndf['Relation'] = df['Relation'].map(label_dict)\ndfv['Relation'] = dfv['Relation'].map(label_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:22.085428Z","iopub.execute_input":"2021-12-12T22:47:22.085712Z","iopub.status.idle":"2021-12-12T22:47:22.101243Z","shell.execute_reply.started":"2021-12-12T22:47:22.085676Z","shell.execute_reply":"2021-12-12T22:47:22.100544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nSpliting up the data randomly into test and validation set.\nAs of now, Gold Standard dataset is not available. \nHence we consider training data for both training and validation and validation data as a test data.\n'''\nX_train, X_val, y_train, y_val = train_test_split(df.index.values, df.Relation.values, test_size=0.01, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:22.103723Z","iopub.execute_input":"2021-12-12T22:47:22.1039Z","iopub.status.idle":"2021-12-12T22:47:22.113433Z","shell.execute_reply.started":"2021-12-12T22:47:22.103878Z","shell.execute_reply":"2021-12-12T22:47:22.112637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here they also add a label to indicate whether it is a training, validation or test set item.\n# if you do cross-validation, you do not need such labeling.\ndf['data_type'] = ['not_set']*df.shape[0]\n\ndf.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'\n\ndf.groupby(['Relation', 'data_type']).count()","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:22.114656Z","iopub.execute_input":"2021-12-12T22:47:22.114964Z","iopub.status.idle":"2021-12-12T22:47:22.24968Z","shell.execute_reply.started":"2021-12-12T22:47:22.114927Z","shell.execute_reply":"2021-12-12T22:47:22.248837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nPre-trained model types depending on the vocabulary size. We do not use the entire list of pre-trained configuration. \nFor more information, refer to the following link: \nhttps://huggingface.co/transformers/_modules/transformers/models/xlm_roberta/tokenization_xlm_roberta.html#XLMRobertaTokenizer\n\nChoose from 2 different model-types: 'xlm-roberta-base' or 'xlm-roberta-large'\n'''\nmodel_type = 'xlm-roberta-base'\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_type)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:22.251005Z","iopub.execute_input":"2021-12-12T22:47:22.2513Z","iopub.status.idle":"2021-12-12T22:47:27.744779Z","shell.execute_reply.started":"2021-12-12T22:47:22.251261Z","shell.execute_reply":"2021-12-12T22:47:27.744037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nRefer https://github.com/huggingface/tokenizers/issues/247 for more information regarding what special tokens are meant to do and \nhow fine-tuning the model can help create better embeddings with these newly added tokens.\n'''\n# special_tokens_dict = {'additional_special_tokens': ['<e1>', '</e1>', '<e2>', '</e2>']}\n# tokenizer.add_special_tokens(special_tokens_dict)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:27.747005Z","iopub.execute_input":"2021-12-12T22:47:27.747444Z","iopub.status.idle":"2021-12-12T22:47:27.753765Z","shell.execute_reply.started":"2021-12-12T22:47:27.747408Z","shell.execute_reply":"2021-12-12T22:47:27.752855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# '''\n# Another pre-processing step before we can train the model.\n# Model training or fine-tuning requires the encoding of the data to the respective tensor format.\n# This function takes in the data as an arguments, encodes the data and returns the data in the tensor format.\n# '''\n# def encode_data(data):\n#     encoded_data = tokenizer.batch_encode_plus(\n#                         data,\n#                         add_special_tokens=False,\n        \n#                         max_length=512,\n#                         padding='max_length',\n#                         truncation=True,\n\n#                         return_attention_mask=True,\n#                         return_tensors='pt'\n#                     )\n#     return encoded_data","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:27.755176Z","iopub.execute_input":"2021-12-12T22:47:27.755618Z","iopub.status.idle":"2021-12-12T22:47:27.763595Z","shell.execute_reply.started":"2021-12-12T22:47:27.755582Z","shell.execute_reply":"2021-12-12T22:47:27.762807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# '''\n# Another pre-processing step before training the model.\n# Model training or fine-tuning requires the encoding of the data to the respective tensor format.\n# Hence, the data is passed on to 'encode_data' function, which returns the data in the respective tensor format.\n# '''\n# encoded_data_training = encode_data(df.Sentence.values)\n# encoded_data_val = encode_data(dfv.Sentence.values)\n# encoded_data_test = encode_data(dft.Sentence.values)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:27.766239Z","iopub.execute_input":"2021-12-12T22:47:27.766621Z","iopub.status.idle":"2021-12-12T22:47:27.775253Z","shell.execute_reply.started":"2021-12-12T22:47:27.766588Z","shell.execute_reply":"2021-12-12T22:47:27.774482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_data_training = tokenizer.batch_encode_plus(\n    df[df['data_type'] == 'train'].Sentence.values,\n    add_special_tokens=False,\n    \n    max_length=512,\n    padding='max_length',\n    truncation=True,\n\n    return_attention_mask=True,\n    return_tensors='pt'\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    df[df['data_type'] == 'val'].Sentence.values,\n    add_special_tokens=False,\n    \n    max_length=512,\n    padding='max_length',\n    truncation=True,\n\n    return_attention_mask=True,\n    return_tensors='pt'\n)\n\nencoded_data_test = tokenizer.batch_encode_plus(\n    dft.Sentence.values,\n    add_special_tokens=False,\n    \n    max_length=512,\n    padding='max_length',\n    truncation=True,\n\n    return_attention_mask=True,\n    return_tensors='pt'\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:27.776814Z","iopub.execute_input":"2021-12-12T22:47:27.777346Z","iopub.status.idle":"2021-12-12T22:47:56.556442Z","shell.execute_reply.started":"2021-12-12T22:47:27.77731Z","shell.execute_reply":"2021-12-12T22:47:56.555702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids_train = encoded_data_training['input_ids']\nattention_masks_train = encoded_data_training['attention_mask']\nlabels_train = torch.tensor(df[df['data_type'] == 'train'].Relation.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df[df['data_type'] == 'val'].Relation.values)\n\ninput_ids_test = encoded_data_test['input_ids']\nattention_masks_test = encoded_data_test['attention_mask']","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.558041Z","iopub.execute_input":"2021-12-12T22:47:56.558329Z","iopub.status.idle":"2021-12-12T22:47:56.581486Z","shell.execute_reply.started":"2021-12-12T22:47:56.558292Z","shell.execute_reply":"2021-12-12T22:47:56.580866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nFetch different layers of the encoded data required for training and prediction\nThis function takes in \n    'encoded_data', 'dataframe' and 'type' of data to \n        return \n    'input_ids', 'attention_mask' and 'labels' where applicable.\n'''\ndef fetch_data_layers(encoded_data, dataframe, type):\n    if type=='train' or type=='val':\n        input_ids = encoded_data['input_ids']\n        attention_masks = encoded_data['attention_mask']\n        labels = torch.tensor(dataframe.Relation.values)\n        return input_ids, attention_masks, labels\n    else:\n        input_ids = encoded_data['input_ids']\n        attention_masks = encoded_data['attention_mask']\n        return input_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.582786Z","iopub.execute_input":"2021-12-12T22:47:56.58303Z","iopub.status.idle":"2021-12-12T22:47:56.589081Z","shell.execute_reply.started":"2021-12-12T22:47:56.582998Z","shell.execute_reply":"2021-12-12T22:47:56.588321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# '''\n# The 'encoded_data', 'dataframe' and 'type' of data are passed to function 'fetch_data_layers' which returns \n# 'input_ids', 'attention_mask' and 'labels' where applicable.\n# '''\n# input_ids_train, attention_masks_train, labels_train = fetch_data_layers(encoded_data_training, df,  'train')\n# input_ids_val, attention_masks_val, labels_val = fetch_data_layers(encoded_data_val, dfv,  'val')\n# input_ids_test, attention_masks_test = fetch_data_layers(encoded_data_test, dft,  'test')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.590315Z","iopub.execute_input":"2021-12-12T22:47:56.591088Z","iopub.status.idle":"2021-12-12T22:47:56.600188Z","shell.execute_reply.started":"2021-12-12T22:47:56.591052Z","shell.execute_reply":"2021-12-12T22:47:56.599444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nBasic configuration for training the model\n'''\nbatch_size = 8\nepochs = 5","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.601479Z","iopub.execute_input":"2021-12-12T22:47:56.601743Z","iopub.status.idle":"2021-12-12T22:47:56.609561Z","shell.execute_reply.started":"2021-12-12T22:47:56.601712Z","shell.execute_reply":"2021-12-12T22:47:56.608912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(labels_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.610801Z","iopub.execute_input":"2021-12-12T22:47:56.611206Z","iopub.status.idle":"2021-12-12T22:47:56.619027Z","shell.execute_reply.started":"2021-12-12T22:47:56.611144Z","shell.execute_reply":"2021-12-12T22:47:56.618333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreating the dataset from the fetched layers\n'''\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\ndataset_test = TensorDataset(input_ids_test, attention_masks_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.620348Z","iopub.execute_input":"2021-12-12T22:47:56.620682Z","iopub.status.idle":"2021-12-12T22:47:56.627534Z","shell.execute_reply.started":"2021-12-12T22:47:56.620648Z","shell.execute_reply":"2021-12-12T22:47:56.626863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nCreating the dataloader by randomly sampling using the dataset previously created through the fetched layers.\n'''\ndataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\ndataloader_val = DataLoader(dataset_val, sampler=RandomSampler(dataset_val), batch_size=batch_size)\ndataloader_test = DataLoader(dataset_test, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.629055Z","iopub.execute_input":"2021-12-12T22:47:56.629641Z","iopub.status.idle":"2021-12-12T22:47:56.636183Z","shell.execute_reply.started":"2021-12-12T22:47:56.629588Z","shell.execute_reply":"2021-12-12T22:47:56.635463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nLoading the pre-trained 'XLM-RoBERTa' model and passing the number of labels along with it as a configuration step.\n'''\n# config = XLMRobertaConfig.from_pretrained(model_type)\nmodel = XLMRobertaForSequenceClassification.from_pretrained(model_type, num_labels=len(label_dict))\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.load_state_dict(torch.load('/kaggle/input/model-checkpoint/Roberta_Model_Checkpoint.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:47:56.637535Z","iopub.execute_input":"2021-12-12T22:47:56.638018Z","iopub.status.idle":"2021-12-12T22:49:05.709699Z","shell.execute_reply.started":"2021-12-12T22:47:56.637983Z","shell.execute_reply":"2021-12-12T22:49:05.709015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Optional, Sequence\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nsoftmax_layer = nn.LogSoftmax(dim=1)\n\n# class FocalLoss(nn.modules.loss._WeightedLoss):\n#     def __init__(self, weight=None, gamma=2, device='cpu'):\n#         super(FocalLoss, self).__init__(weight)\n#         # focusing hyper-parameter gamma\n#         self.gamma = gamma\n\n#         # class weights will act as the alpha parameter\n#         self.weight = weight\n        \n#         # using deivce (cpu or gpu)\n#         self.device = device\n        \n#         self.ce_loss = nn.CrossEntropyLoss()\n\n#     def forward(self, _input, _target):\n#         focal_loss = 0\n\n#         for i in range(len(_input)):\n#             # -log(pt)\n#             cur_ce_loss = self.ce_loss(_input[i].view(-1, _input[i].size()[-1]), _target[i].view(-1))\n#             # pt\n#             pt = torch.exp(-cur_ce_loss)\n\n#             if self.weight is not None:\n#                 # alpha * (1-pt)^gamma * -log(pt)\n#                 cur_focal_loss = self.weight[_target[i]] * ((1 - pt) ** self.gamma) * cur_ce_loss\n#             else:\n#                 # (1-pt)^gamma * -log(pt)\n#                 cur_focal_loss = ((1 - pt) ** self.gamma) * cur_ce_loss\n                \n#             focal_loss = focal_loss + cur_focal_loss\n\n#         if self.weight is not None:\n#             focal_loss = focal_loss / self.weight.sum()\n#             return focal_loss.to(self.device)\n        \n#         focal_loss = focal_loss / torch.tensor(len(probs))    \n#         return focal_loss.to(self.device)\n\nclass FocalLoss(nn.Module):\n    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n    It is essentially an enhancement to cross entropy loss and is\n    useful for classification tasks when there is a large class imbalance.\n    x is expected to contain raw, unnormalized scores for each class.\n    y is expected to contain class labels.\n    Shape:\n        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n    \"\"\"\n\n    def __init__(self,\n                 alpha: Optional[Tensor] = None,\n                 gamma: float = 0.,\n                 reduction: str = 'mean',\n                 ignore_index: int = -100):\n        \"\"\"Constructor.\n        Args:\n            alpha (Tensor, optional): Weights for each class. Defaults to None.\n            gamma (float, optional): A constant, as described in the paper.\n                Defaults to 0.\n            reduction (str, optional): 'mean', 'sum' or 'none'.\n                Defaults to 'mean'.\n            ignore_index (int, optional): class label to ignore.\n                Defaults to -100.\n        \"\"\"\n        if reduction not in ('mean', 'sum', 'none'):\n            raise ValueError(\n                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n\n        self.nll_loss = nn.NLLLoss(\n            weight=alpha, reduction='none', ignore_index=ignore_index)\n\n    def __repr__(self):\n        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n        arg_vals = [self.__dict__[k] for k in arg_keys]\n        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n        arg_str = ', '.join(arg_strs)\n        return f'{type(self).__name__}({arg_str})'\n\n    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n        if x.ndim > 2:\n            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n            c = x.shape[1]\n            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n            y = y.view(-1)\n\n        unignored_mask = y != self.ignore_index\n        y = y[unignored_mask]\n        if len(y) == 0:\n            return 0.\n        x = x[unignored_mask]\n\n        # compute weighted cross entropy term: -alpha * log(pt)\n        # (alpha is already part of self.nll_loss)\n        log_p = F.log_softmax(x, dim=-1)\n        ce = self.nll_loss(log_p, y)\n\n        # get true class column from each row\n        all_rows = torch.arange(len(x))\n        log_pt = log_p[all_rows, y]\n\n        # compute focal term: (1 - pt)^gamma\n        pt = log_pt.exp()\n        focal_term = (1 - pt)**self.gamma\n\n        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n        loss = focal_term * ce\n\n        if self.reduction == 'mean':\n            loss = loss.mean()\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n\n        return loss\n\n\ndef focal_loss(alpha: Optional[Sequence] = None,\n               gamma: float = 0.,\n               reduction: str = 'mean',\n               ignore_index: int = -100,\n               device='cpu',\n               dtype=torch.float32) -> FocalLoss:\n    \"\"\"Factory function for FocalLoss.\n    Args:\n        alpha (Sequence, optional): Weights for each class. Will be converted\n            to a Tensor if not None. Defaults to None.\n        gamma (float, optional): A constant, as described in the paper.\n            Defaults to 0.\n        reduction (str, optional): 'mean', 'sum' or 'none'.\n            Defaults to 'mean'.\n        ignore_index (int, optional): class label to ignore.\n            Defaults to -100.\n        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n        dtype (torch.dtype, optional): dtype to cast alpha to.\n            Defaults to torch.float32.\n    Returns:\n        A FocalLoss object\n    \"\"\"\n    if alpha is not None:\n        if not isinstance(alpha, Tensor):\n            alpha = torch.tensor(alpha)\n        alpha = alpha.to(device=device, dtype=dtype)\n\n    fl = FocalLoss(\n        alpha=alpha,\n        gamma=gamma,\n        reduction=reduction,\n        ignore_index=ignore_index)\n    return fl\n    \nentropy_loss = focal_loss(alpha=None,gamma=2,device='cuda')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:05.711154Z","iopub.execute_input":"2021-12-12T22:49:05.711412Z","iopub.status.idle":"2021-12-12T22:49:05.77061Z","shell.execute_reply.started":"2021-12-12T22:49:05.711379Z","shell.execute_reply":"2021-12-12T22:49:05.769907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nBasic configuration for training the model\n'''\noptimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:05.771705Z","iopub.execute_input":"2021-12-12T22:49:05.771981Z","iopub.status.idle":"2021-12-12T22:49:06.743804Z","shell.execute_reply.started":"2021-12-12T22:49:05.771927Z","shell.execute_reply":"2021-12-12T22:49:06.742798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:06.745591Z","iopub.execute_input":"2021-12-12T22:49:06.74587Z","iopub.status.idle":"2021-12-12T22:49:06.751136Z","shell.execute_reply.started":"2021-12-12T22:49:06.745834Z","shell.execute_reply":"2021-12-12T22:49:06.750333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_per_class(preds, labels):\n    label_dict_inverse = {v:k for k,v in label_dict.items()}\n\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:06.75795Z","iopub.execute_input":"2021-12-12T22:49:06.758174Z","iopub.status.idle":"2021-12-12T22:49:06.764512Z","shell.execute_reply.started":"2021-12-12T22:49:06.758129Z","shell.execute_reply":"2021-12-12T22:49:06.763642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_val = 123\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:06.766243Z","iopub.execute_input":"2021-12-12T22:49:06.766592Z","iopub.status.idle":"2021-12-12T22:49:06.773733Z","shell.execute_reply.started":"2021-12-12T22:49:06.766554Z","shell.execute_reply":"2021-12-12T22:49:06.772973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = xm.xla_device()\nmodel.to(device)\n\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:06.776184Z","iopub.execute_input":"2021-12-12T22:49:06.776606Z","iopub.status.idle":"2021-12-12T22:49:07.075574Z","shell.execute_reply.started":"2021-12-12T22:49:06.776573Z","shell.execute_reply":"2021-12-12T22:49:07.074858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(dataloader_test, relation_tensor):\n\n    model.eval()\n    \n    predictions = []\n    \n    for batch in tqdm(dataloader_test):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1]\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        logits = outputs['logits']\n        \n        relation_tensor = relation_tensor.type(torch.DoubleTensor)\n        relation_tensor = relation_tensor.to(device)\n        \n#         print(logits)\n        \n        logits = logits.type(torch.DoubleTensor)\n        logits = logits.to(device)\n        new_logits = torch.tensor([])\n        new_logits = new_logits.to(device)\n        new_logits = logits * relation_tensor\n\n        new_logits = new_logits.detach().cpu().numpy()\n        predictions.append(new_logits)\n    \n    predictions = np.concatenate(predictions, axis=0)\n            \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:07.076949Z","iopub.execute_input":"2021-12-12T22:49:07.077205Z","iopub.status.idle":"2021-12-12T22:49:07.086922Z","shell.execute_reply.started":"2021-12-12T22:49:07.077157Z","shell.execute_reply":"2021-12-12T22:49:07.086208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader_val, relation_tensor):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2]\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n            \n        relation_tensor = relation_tensor.type(torch.DoubleTensor)\n        relation_tensor = relation_tensor.to(device)\n        \n#         print(logits)\n        \n        logits = logits.type(torch.DoubleTensor)\n        logits = logits.to(device)\n        new_logits = torch.tensor([])\n        new_logits = new_logits.to(device)\n        \n        new_logits = logits * relation_tensor\n        ll = entropy_loss(new_logits, inputs['labels'])\n\n        loss_val_total += ll.item()\n        \n\n        new_logits = new_logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(new_logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:07.087865Z","iopub.execute_input":"2021-12-12T22:49:07.091132Z","iopub.status.idle":"2021-12-12T22:49:07.101435Z","shell.execute_reply.started":"2021-12-12T22:49:07.091104Z","shell.execute_reply":"2021-12-12T22:49:07.100771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_tensor","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:07.103194Z","iopub.execute_input":"2021-12-12T22:49:07.10386Z","iopub.status.idle":"2021-12-12T22:49:07.11611Z","shell.execute_reply.started":"2021-12-12T22:49:07.10377Z","shell.execute_reply":"2021-12-12T22:49:07.115402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(1, epochs+1)):\n    model.train()\n    loss_train_total = 0\n    best_validation_loss = float('inf')\n    \n    progress_bar = tqdm(dataloader_train, \n                        desc='Epoch {:1d}'.format(epoch), \n                        leave=False, \n                        disable=False)\n    \n    for batch in progress_bar:\n        model.zero_grad()\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {\n            'input_ids':        batch[0],\n            'attention_mask':   batch[1],\n            'labels':           batch[2],\n        }\n        # inputs['input_ids'].to(device)\n        # inputs['attention_mask'].to(device)\n        # inputs['labels'].to(device)\n\n        # outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=inputs['labels'])\n        \n#         print(inputs['labels'])\n        \n        outputs = model(**inputs)        \n        loss = outputs[0]\n        logits = outputs[1]\n        relation_tensor = relation_tensor.type(torch.DoubleTensor)\n        relation_tensor = relation_tensor.to(device)\n        \n#         print(logits)\n        \n        logits = logits.type(torch.DoubleTensor)\n        logits = logits.to(device)\n        new_logits = torch.tensor([])\n        new_logits = new_logits.to(device)\n        new_logits = logits * relation_tensor\n\n        \n        ll = entropy_loss(new_logits, inputs['labels'])\n        \n        # loss_train_total +=loss.item()\n        loss_train_total +=ll.item()\n        # loss.backward()\n        ll.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        \n        # progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(ll.item()/len(batch))})    \n    \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total/len(dataloader_train)\n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_val, relation_tensor)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (weighted): {val_f1}')\n    \n    if val_loss <= best_validation_loss:\n        torch.save(model.state_dict(), 'Roberta_Model_Checkpoint.pth')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T22:49:07.119357Z","iopub.execute_input":"2021-12-12T22:49:07.119907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_per_class(predictions, true_vals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = predict(dataloader_test, relation_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_flat = np.argmax(predictions, axis=1).flatten()\n# len(preds_flat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict_rev = dict((v,k) for k,v in label_dict.items())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dft['predicted_relations_id'] = preds_flat\ndft['predicted_relations'] = dft['predicted_relations_id'].map(label_dict_rev)\ndft.to_csv('Roberta_Without_Tokens_2.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dft['predicted_relations']","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}